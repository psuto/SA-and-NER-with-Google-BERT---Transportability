{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_dir = 'C:\\\\Users\\\\Phil\\\\Sync\\\\entity-recognition-datasets-master\\\\sentiment\\\\Data\\\\IMDB Reviews\\\\IMDB Data\\\\test'\n",
    "train_dir = 'C:\\\\Users\\\\Phil\\\\Sync\\\\entity-recognition-datasets-master\\\\sentiment\\\\Data\\\\IMDB Reviews\\\\IMDB Data\\\\train'\n",
    "\n",
    "# Alternatively written as a function for importing from different directory sources\n",
    "def IMDB_to_csv( directory ):    \n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for filename in glob.glob(str(directory)+'\\\\neg\\\\*.txt'):\n",
    "        with open(filename, 'r',  encoding=\"utf8\") as f:\n",
    "            content = f.readlines()\n",
    "            content_table = pd.DataFrame({'id':filename.split('_')[0].split('\\\\')[-1],'rating':filename.split('_')[1].split('.')[0],'pol':'neg', 'text':content})\n",
    "        data = data.append(content_table)\n",
    "        \n",
    "    for filename in glob.glob(str(directory)+'\\\\pos\\\\*.txt'):\n",
    "        with open(filename, 'r',  encoding=\"utf8\") as f:\n",
    "            content = f.readlines()\n",
    "            content_table = pd.DataFrame({'id':filename.split('_')[0].split('\\\\')[-1],'rating':filename.split('_')[1].split('.')[0],'pol':'pos', 'text':content})\n",
    "        data = data.append(content_table)\n",
    "    data = data.sort_values(['pol','id'])\n",
    "    data = data.reset_index(drop=True)\n",
    "    #data['rating_norm'] = (data['rating'] - data['rating'].min())/( data['rating'].max() - data['rating'].min() )\n",
    "\n",
    "    return(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pol'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-083d19436d00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pydev_debug_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mIMDB_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMDB_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m IMDB_train['pol_id'] = np.where(IMDB_train['pol']=='neg',-1,\n\u001b[1;32m      4\u001b[0m                         np.where(IMDB_train['pol']=='pos',1,0))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-6bca52a4dce1>\u001b[0m in \u001b[0;36mIMDB_to_csv\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mcontent_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pol'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pol'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#data['rating_norm'] = (data['rating'] - data['rating'].min())/( data['rating'].max() - data['rating'].min() )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-tensorflow-py37/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position)\u001b[0m\n\u001b[1;32m   4999\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlexsort_indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5001\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5002\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexsort_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5003\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-tensorflow-py37/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4999\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlexsort_indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5001\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5002\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexsort_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5003\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/bert-tensorflow-py37/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1772\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1774\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pol'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "IMDB_train = IMDB_to_csv(train_dir)\n",
    "IMDB_train['pol_id'] = np.where(IMDB_train['pol']=='neg',-1,\n",
    "                        np.where(IMDB_train['pol']=='pos',1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "IMDB_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "IMDB_test = IMDB_to_csv(test_dir)\n",
    "IMDB_test['pol_id'] = np.where(IMDB_test['pol']=='neg',-1,\n",
    "                        np.where(IMDB_test['pol']=='pos',1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "IMDB_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "rt_train_path = 'C:\\\\Users\\\\Phil\\\\Sync\\\\entity-recognition-datasets-master\\\\sentiment\\\\Data\\\\RT_Sentiment\\\\train.tsv'\n",
    "rt_train_data = pd.read_csv(rt_train_path,header=0,delimiter=\"\\t\",quoting=3)\n",
    "rt_train_data['pol'] = np.where(rt_train_data['Sentiment']==3,\"neut\", np.where(rt_train_data['Sentiment']<3,\"neg\", np.where(rt_train_data['Sentiment']>3,\"pos\",\"\")))\n",
    "# Remove any neutral classified phrases\n",
    "rt_train_data = rt_train_data[rt_train_data['pol']!=\"neut\"]\n",
    "rt_train_data = rt_train_data[rt_train_data['pol']!=\"\"]\n",
    "\n",
    "rt_train_data = rt_train_data.reset_index(drop=True)\n",
    "#rt_test_path = 'E:\\\\Documents\\\\Text Data\\\\rottentomatoes\\\\test.tsv'\n",
    "#rt_test_data = pd.read_csv(rt_test_path,header=0,delimiter=\"\\t\",quoting=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "count_list = pd.DataFrame()\n",
    "for i in range(0,len(rt_train_data)):\n",
    "    count = pd.DataFrame({'count':len(rt_train_data['Phrase'][i].split())},index=[i])\n",
    "    count_list = count_list.append(count)\n",
    "count_list = count_list.reset_index(drop=True)\n",
    "rt_train_data['word_count'] = count_list['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "rt_train_data_2 = pd.DataFrame()\n",
    "for i in range(1,max(rt_train_data['SentenceId'])):\n",
    "    #Some sentence ids are not used and so we skip these\n",
    "    if (len(rt_train_data[rt_train_data['SentenceId'] == i])==0):\n",
    "        continue\n",
    "    else:\n",
    "        rt_train_data_2 = rt_train_data_2.append(rt_train_data[rt_train_data['SentenceId'] == i].sort_values('word_count', ascending=False).reset_index(drop=True).iloc[0,:])\n",
    "rt_train_data_2 = rt_train_data_2.reset_index(drop=True)\n",
    "# rename column from phrase to text for clarity\n",
    "rt_train_data_2.columns = ['text', 'textId', 'SentenceId', 'Sentiment', 'pol', 'word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "rt_train_data_2['pol_id'] = np.where(rt_train_data_2['pol']=='neg',-1,\n",
    "                        np.where(rt_train_data_2['pol']=='pos',1,0))\n",
    "rt_train_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "twitter_train_path = 'C:\\\\Users\\\\Phil\\\\Sync\\\\entity-recognition-datasets-master\\\\sentiment\\\\Data\\\\twitter\\\\train.csv'\n",
    "twitter_train = pd.read_csv(twitter_train_path, encoding=\"ISO-8859-1\")\n",
    "twitter_train.columns = ['ItemID','pol','text']\n",
    "twitter_train['pol_id'] = np.where(twitter_train['pol']=='neg',-1,\n",
    "                        np.where(twitter_train['pol']=='pos',1,0))\n",
    "twitter_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "twitter_products_path = 'C:\\\\Users\\\\Phil\\\\Sync\\\\entity-recognition-datasets-master\\\\sentiment\\\\Data\\\\twitter\\\\judge-1377884607_tweet_product_company.csv'\n",
    "twitter_products = pd.read_csv(twitter_products_path, encoding=\"ISO-8859-1\")\n",
    "twitter_products.columns = ['text','product','pol']\n",
    "twitter_products['pol_id'] = np.where(twitter_products['pol']=='Negative emotion',-1,\n",
    "                        np.where(twitter_products['pol']=='Positive emotion',1,0))\n",
    "twitter_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "finance_messages_path = 'C:\\\\Users\\\\Phil\\\\Sync\\\\entity-recognition-datasets-master\\\\sentiment\\\\Data\\\\finance\\\\EnglishGS.csv'\n",
    "finance_messages = pd.read_csv(finance_messages_path)\n",
    "finance_messages.columns = ['unique_id','text','pol','type','id']\n",
    "\n",
    "finance_messages['pol_id'] = np.where(finance_messages['pol']<0,-1,\n",
    "                        np.where(finance_messages['pol']>0,1,0))\n",
    "finance_messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "finance_headlines_path = 'C:\\\\Users\\\\Phil\\\\Sync\\\\entity-recognition-datasets-master\\\\sentiment\\\\Data\\\\finance\\\\SSIX News headlines Gold Standard EN.csv'\n",
    "finance_headlines = pd.read_csv(finance_headlines_path)\n",
    "finance_headlines.columns = ['unique_id','company','company_fixed','text','pol','num_scores']\n",
    "\n",
    "finance_headlines['pol_id'] = np.where(finance_headlines['pol']<0,-1,\n",
    "                        np.where(finance_headlines['pol']>0,1,0))\n",
    "finance_headlines.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply TextBlob pre-trained Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "text = '''\n",
    "The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.\n",
    "'''\n",
    "\n",
    "blob = TextBlob(text)\n",
    "                                      \n",
    "for sentence in blob.sentences:\n",
    "    print(sentence.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def text_blob(data_column):\n",
    "    output_labels = pd.DataFrame()\n",
    "    for n,phrases in enumerate(data_column):\n",
    "        blob = TextBlob(phrases)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        \n",
    "        if polarity > 0:\n",
    "            label = 1\n",
    "        elif polarity == 0:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = -1\n",
    "        \n",
    "        output_labels = output_labels.append(pd.DataFrame({'label':label},index=[n]))\n",
    "        \n",
    "    return(output_labels['label'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "IMDB_train['text'][0].split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for n,phrases in enumerate(IMDB_train['text'][0:1]):\n",
    "    blob = TextBlob(phrases)\n",
    "    phrase_sentence_pol = pd.DataFrame()\n",
    "    print(blob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "IMDB_train['text_blob'] = text_blob(IMDB_train['text'])\n",
    "IMDB_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "IMDB_train['pol_id'] = np.where(IMDB_train['pol']=='neg',-1,\n",
    "                        np.where(IMDB_train['pol']=='pos',1,0))\n",
    "IMDB_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "precision_applied = sklearn.metrics.precision_score(IMDB_train['pol_id'],\n",
    "                                                            IMDB_train['text_blob'], average='weighted')\n",
    "recall_applied = sklearn.metrics.recall_score(IMDB_train['pol_id'],\n",
    "                                                            IMDB_train['text_blob'], average='weighted')\n",
    "\n",
    "F1_applied = 2 * (precision_applied * recall_applied) / (precision_applied + recall_applied)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F1_applied' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fe02de7b46f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF1_applied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F1_applied' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "F1_applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "blob = TextBlob(\"neutral\", analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment.classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def text_blob_NB(data_column):\n",
    "    output_labels = pd.DataFrame()\n",
    "    for n,phrases in enumerate(data_column):\n",
    "        clear_output(wait=True)\n",
    "        print(\"Completed:\", np.round(n/len(data_column)*100),\"%\")\n",
    "        blob = TextBlob(phrases,analyzer=NaiveBayesAnalyzer())\n",
    "        polarity = blob.sentiment.classification\n",
    "        \n",
    "        if polarity == 'pos':\n",
    "            label = 1\n",
    "        elif polarity == 'neg':\n",
    "            label = -1\n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        output_labels = output_labels.append(pd.DataFrame({'label':label},index=[n]))\n",
    "        \n",
    "    return(output_labels['label'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "IMDB_train['text_blob'] = text_blob_NB(IMDB_train['text'][0:10])\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "\n",
    "IMDB_train['pol_id'] = np.where(IMDB_train['pol']=='neg',-1,\n",
    "                        np.where(IMDB_train['pol']=='pos',1,0))\n",
    "precision_applied = sklearn.metrics.precision_score(IMDB_train['pol_id'],\n",
    "                                                            IMDB_train['text_blob'], average='weighted')\n",
    "recall_applied = sklearn.metrics.recall_score(IMDB_train['pol_id'],\n",
    "                                                            IMDB_train['text_blob'], average='weighted')\n",
    "\n",
    "F1_applied = 2 * (precision_applied * recall_applied) / (precision_applied + recall_applied)\n",
    "F1_applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to all datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def text_blob_2(data_column):\n",
    "\n",
    "    output_labels = pd.DataFrame()\n",
    "    for n,phrases in enumerate(data_column):\n",
    "        blob = TextBlob(phrases)\n",
    "        polarity = blob.sentiment.polarity\n",
    "\n",
    "        if polarity > 0:\n",
    "            label = 1\n",
    "        elif polarity == 0:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = -1\n",
    "\n",
    "        output_labels = output_labels.append(pd.DataFrame({'label':label},index=[n]))\n",
    "    return(output_labels['label'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    output_labels_NB = pd.DataFrame()\n",
    "    for n,phrases in enumerate(data_column):\n",
    "        blob = TextBlob(phrases,analyzer=NaiveBayesAnalyzer())\n",
    "        polarity = blob.sentiment.classification\n",
    "\n",
    "        if polarity == 'pos':\n",
    "            label = 1\n",
    "        elif polarity == 'neg':\n",
    "            label = -1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        output_labels_NB = output_labels_NB.append(pd.DataFrame({'label':label},index=[n]))\n",
    "\n",
    "    return(output_labels['label'],output_labels_NB['label'])\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    precision_applied_NB = sklearn.metrics.precision_score(IMDB_train['pol_id'],\n",
    "                                                                IMDB_train['text_blob_NB'], average='weighted')\n",
    "    recall_applied_NB = sklearn.metrics.recall_score(IMDB_train['pol_id'],\n",
    "                                                                IMDB_train['text_blob_NB'], average='weighted')\n",
    "\n",
    "    F1_applied_NB = 2 * (precision_applied_NB * recall_applied_NB) / (precision_applied_NB + recall_applied_NB)\n",
    "    \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "datasets = [IMDB_train,IMDB_test,rt_train_data_2,twitter_products,finance_messages,finance_headlines]\n",
    "dataset_names = ['IMDB_train','IMDB_test','rt_train_data_2','twitter_products','financial_messages','financial_headlines']\n",
    "dataset_text_col = ['text','text','text','text','text','text']\n",
    "\n",
    "output = pd.DataFrame()\n",
    "for n,dataset in enumerate(datasets):\n",
    "    print('Current Dataset:',dataset_names[n])\n",
    "    datasets[n]['text_blob_def'] = text_blob_2(datasets[n][str(dataset_text_col[n])].astype(str))\n",
    "    #dataset['text_blob_NB'] = text_blob_2(dataset[str(dataset_text_col[n])])[0]\n",
    "\n",
    "    precision_applied = sklearn.metrics.precision_score(datasets[n]['pol_id'],\n",
    "                                                                datasets[n]['text_blob_def'], average='weighted')\n",
    "    recall_applied = sklearn.metrics.recall_score(datasets[n]['pol_id'],\n",
    "                                                                datasets[n]['text_blob_def'], average='weighted')\n",
    "\n",
    "    F1_applied = 2 * (precision_applied * recall_applied) / (precision_applied + recall_applied)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    output = output.append(pd.DataFrame({'dataset':dataset_names[n],\n",
    "                                         'text_blob_def_prec':precision_applied,\n",
    "                                         'text_blob_def_recall':recall_applied,\n",
    "                                         'text_blob_def_F1':F1_applied\n",
    "                                         }, index=[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(output['dataset'],output['text_blob_def_F1'])\n",
    "plt.title(\"TextBlob Sentiment Analysis Appied to Datasets (F1)\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylim([0,1])\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def text_blob_2(data_column):\n",
    "\n",
    "    output_labels = pd.DataFrame()\n",
    "    for n,phrases in enumerate(data_column):\n",
    "        blob = TextBlob(phrases)\n",
    "        polarity = blob.sentiment.polarity\n",
    "\n",
    "        if polarity > 0:\n",
    "            label = 1\n",
    "        elif polarity < 0:\n",
    "            label = -1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        output_labels = output_labels.append(pd.DataFrame({'label':label},index=[n]))\n",
    "        \n",
    "    output_labels_NB = pd.DataFrame()\n",
    "    for n,phrases in enumerate(data_column):\n",
    "        clear_output(wait=True)\n",
    "        print(\"Completed:\", np.round(n/len(data_column),4)*100,\"%\")\n",
    "        blob = TextBlob(phrases,analyzer=NaiveBayesAnalyzer())\n",
    "        polarity = blob.sentiment.classification\n",
    "\n",
    "        if polarity == 'pos':\n",
    "            label = 1\n",
    "        elif polarity == 'neg':\n",
    "            label = -1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        output_labels_NB = output_labels_NB.append(pd.DataFrame({'label':label},index=[n]))\n",
    "\n",
    "    return(output_labels['label'],output_labels_NB['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "datasets = [IMDB_train,IMDB_test,rt_train_data_2,twitter_train]\n",
    "dataset_names = ['IMDB_train','IMDB_test','rt_train_data_2','twitter_train']\n",
    "dataset_text_col = ['text','text','text','text']\n",
    "\n",
    "output = pd.DataFrame()\n",
    "for n,dataset in enumerate(datasets[1:2]):\n",
    "    print('Current Dataset:',dataset_names[n])\n",
    "    prediction = text_blob_2(datasets[n][str(dataset_text_col[n])])\n",
    "    datasets[n]['text_blob_def'] = prediction[0]\n",
    "    dataset['text_blob_NB'] = prediction[1]\n",
    "\n",
    "    precision_applied = sklearn.metrics.precision_score(datasets[n]['pol_id'],\n",
    "                                                                datasets[n]['text_blob_def'], average='weighted')\n",
    "    recall_applied = sklearn.metrics.recall_score(datasets[n]['pol_id'],\n",
    "                                                                datasets[n]['text_blob_def'], average='weighted')\n",
    "\n",
    "    F1_applied = 2 * (precision_applied * recall_applied) / (precision_applied + recall_applied)\n",
    "    \n",
    "    precision_applied_NB = sklearn.metrics.precision_score(datasets[n]['pol_id'],\n",
    "                                                                datasets[n]['text_blob_NB'], average='weighted')\n",
    "    recall_applied_NB = sklearn.metrics.recall_score(datasets[n]['pol_id'],\n",
    "                                                                datasets[n]['text_blob_NB'], average='weighted')\n",
    "\n",
    "    F1_applied_NB = 2 * (precision_applied_NB * recall_applied_NB) / (precision_applied_NB + recall_applied_NB)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    output = output.append(pd.DataFrame({'dataset':dataset_names[n],\n",
    "                                         'text_blob_def_prec':precision_applied,\n",
    "                                         'text_blob_def_recall':recall_applied,\n",
    "                                         'text_blob_def_F1':F1_applied,\n",
    "                                         'text_blob_NB_prec':precision_applied_NB,\n",
    "                                         'text_blob_NB_recall':recall_applied_NB,\n",
    "                                         'text_blob_NB_F1':F1_applied_NB\n",
    "                                         }, index=[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
