{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "%cd%\r\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "!echo %cd% # under windows\n",
    "#!pwd # under linux/mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "**Wikigold**\n",
    "\n",
    "- Labels: 'I-MISC', 'O', 'I-ORG', 'I-PER', 'I-LOC'\n",
    "- Split into train/test set by taking first 80% of words for train and last 20% of words for test (to preserve sentences)\n",
    "\n",
    "**Twitter (train/test)**\n",
    "\n",
    "- Labels: 'O', 'B-location', 'I-location', 'B-group', 'B-corporation', 'B-person', 'B-creative-work', 'B-product', 'I-person', 'I-creative-work', 'I-corporation', 'I-group', 'I-product'\n",
    "- Map labels to match wikigold, both I/B tags are mapped to I\n",
    "- Remove punctuation\n",
    "\n",
    "**Financial SEC-filings (train/test)**\n",
    "\n",
    "- Labels: 'O' 'I-ORG' 'I-LOC' 'I-PER' 'I-MISC'\n",
    "- Remove punctuation\n",
    "\n",
    "**Anatomical AnEM (train/test)**\n",
    "\n",
    "- Labels: 'B-Multi-tissue_structure', 'O', 'B-Organism_substance', 'B-Organism_subdivision', 'B-Organ', 'I-Multi-tissue_structure', 'B-Cellular_component', 'I-Cellular_component', 'B-Cell', 'I-Cell', 'B-Immaterial_anatomical_entity', 'B-Tissue', 'I-Tissue',\n",
    "'B-Pathological_formation', 'B-Anatomical_system', 'I-Organism_substance', 'I-Anatomical_system', 'I-Pathological_formation', 'I-Immaterial_anatomical_entity', 'I-Organ', 'I-Organism_subdivision', 'B-Developing_anatomical_structure', 'I-Developing_anatomical_structure'\n",
    "- Remove punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B- denotes the beginning and I- inside of an entity. The prefixes are used to detect multiword entities, as shown in the example example above. All other words, which donâ€™t refer to entities of interest, are labelled with the O tag.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Wikigold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-102fe11522c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#wiki = pd.read_csv('C:/Users/Phil/Sync/entity-recognition-datasets-master/data/wikigold/CONLL-format/data/wikigold.conll.txt', sep=\" \", header=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# wiki = pd.read_csv('~/Sync/entity-recognition-datasets-master/data/wikigold/CONLL-format/data/wikigold.conll.txt', sep=\"\\n| \", header=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data/wikigold/CONLL-format/data/wikigold.conll.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n| \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwiki\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/bert-tensorflow-py37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/bert-tensorflow-py37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/bert-tensorflow-py37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/bert-tensorflow-py37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0;34m' \"python-fwf\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 )\n\u001b[0;32m-> 1147\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/bert-tensorflow-py37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2291\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2293\u001b[0;31m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2294\u001b[0m         )\n\u001b[1;32m   2295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/bert-tensorflow-py37/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data/wikigold/CONLL-format/data/wikigold.conll.txt'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data/wikigold/CONLL-format/data/wikigold.conll.txt'",
     "output_type": "error"
    }
   ],
   "source": [
    "#wiki = pd.read_csv('C:/Users/Phil/Sync/entity-recognition-datasets-master/data/wikigold/CONLL-format/data/wikigold.conll.txt', sep=\" \", header=None)\n",
    "# wiki = pd.read_csv('~/Sync/entity-recognition-datasets-master/data/wikigold/CONLL-format/data/wikigold.conll.txt', sep=\"\\n| \", header=None)\n",
    "wiki = pd.read_csv('C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data/wikigold/CONLL-format/data/wikigold.conll.txt', sep=\"\\n| \", header=None)   \n",
    "wiki.columns = ['word','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(wiki['label'].unique())\n",
    "\n",
    "# First remove puncutation and rows that only have punctuation, e.g. \"&\"\n",
    "wiki_2 = wiki.copy()\n",
    "wiki_2['word'] = wiki_2['word'].str.extract('(\\w+)', expand = False)\n",
    "wiki_2 = wiki_2.dropna().reset_index(drop=True)\n",
    "wiki_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#wiki['label'] = np.where(wiki['label']=='O','O','tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "wiki_train = wiki_2.iloc[:int(len(wiki_2)*0.8),:]\n",
    "wiki_test = wiki_2.iloc[int(len(wiki_2)*0.8):,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Twitter WNUT17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#social_media_train = pd.read_csv('C:/Users/Phil/Sync/entity-recognition-datasets-master/data/WNUT17/CONLL-format/data/train/wnut17train.conll', sep=\"\\t\", header=None)\n",
    "# social_media_train = pd.read_csv('~/Sync/entity-recognition-datasets-master/data/WNUT17/CONLL-format/data/train/wnut17train.conll', sep=\"\\t\", header=None)\n",
    "social_media_train = pd.read_csv('C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data/WNUT17/CONLL-format/data/train/wnut17train.conll', sep=\"\\t\", header=None)\n",
    "    \n",
    "social_media_train.columns = ['word','label']\n",
    "print(social_media_train['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Map labels to those consistent from wiki\n",
    "\"\"\"\n",
    "social_media_train['label'] = np.where(social_media_train['label']=='I-location', 'I-LOC',\n",
    "                              np.where(social_media_train['label']=='I-person', 'I-PER',\n",
    "                              np.where(social_media_train['label']=='I-corporation', 'I-ORG',\n",
    "                              np.where(social_media_train['label']=='I-group', 'I-ORG', \n",
    "                              np.where(social_media_train['label']=='I-creative-work', 'I-MISC',\n",
    "                              np.where(social_media_train['label']=='I-product', 'I-MISC', 'O'))))))\n",
    "\"\"\"\n",
    "social_media_train['label'] = np.where((social_media_train['label']=='I-location') | (social_media_train['label']=='B-location'), 'I-LOC',\n",
    "                              np.where((social_media_train['label']=='I-person') | (social_media_train['label']=='B-person'), 'I-PER',\n",
    "                              np.where((social_media_train['label']=='I-corporation') | (social_media_train['label']=='B-corporation'), 'I-ORG',\n",
    "                              np.where((social_media_train['label']=='I-group') | (social_media_train['label']=='B-group'), 'I-ORG', \n",
    "                              np.where((social_media_train['label']=='I-creative-work') | (social_media_train['label']=='B-creative-work'), 'I-MISC',\n",
    "                              np.where((social_media_train['label']=='I-product') | (social_media_train['label']=='B-product'), 'I-MISC', 'O'))))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#social_media_train['label'] = np.where(social_media_train['label']=='O','O','tag')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First remove puncutation and rows that only have punctuation, e.g. \"&\"\n",
    "social_media_train_2 = social_media_train.copy()\n",
    "social_media_train_2['word'] = social_media_train_2['word'].str.extract('(\\w+)', expand = False)\n",
    "social_media_train_2 = social_media_train_2.dropna().reset_index(drop=True)\n",
    "social_media_train_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# social_media_test = pd.read_csv('~/Sync/entity-recognition-datasets-master/data/WNUT17/CONLL-format/data/test/emerging.test.annotated', sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "social_media_test = pd.read_csv(\"C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data/WNUT17/CONLL-format/data/test/emerging.test.annotated\", sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE, encoding='utf-8')  \n",
    "\n",
    "social_media_test.columns = ['word','label']\n",
    "print(social_media_test['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Map labels to those consistent from wiki\n",
    "\"\"\"\n",
    "social_media_test['label'] = np.where(social_media_test['label']=='I-location', 'I-LOC',\n",
    "                              np.where(social_media_test['label']=='I-person', 'I-PER',\n",
    "                              np.where(social_media_test['label']=='I-corporation', 'I-ORG',\n",
    "                              np.where(social_media_test['label']=='I-group', 'I-ORG', \n",
    "                              np.where(social_media_test['label']=='I-creative-work', 'I-MISC',\n",
    "                              np.where(social_media_test['label']=='I-product', 'I-MISC', 'O'))))))\n",
    "\"\"\"\n",
    "\n",
    "social_media_test['label'] = np.where((social_media_test['label']=='I-location') | (social_media_test['label']=='B-location'), 'I-LOC',\n",
    "                              np.where((social_media_test['label']=='I-person') | (social_media_test['label']=='B-person'), 'I-PER',\n",
    "                              np.where((social_media_test['label']=='I-corporation') | (social_media_test['label']=='B-corporation'), 'I-ORG',\n",
    "                              np.where((social_media_test['label']=='I-group') | (social_media_test['label']=='B-group'), 'I-ORG', \n",
    "                              np.where((social_media_test['label']=='I-creative-work') | (social_media_test['label']=='B-creative-work'), 'I-MISC',\n",
    "                              np.where((social_media_test['label']=='I-product') | (social_media_test['label']=='B-product'), 'I-MISC', 'O'))))))\n",
    "\n",
    "\n",
    "\n",
    "#social_media_test['label'] = np.where(social_media_test['label']=='O','O','tag')\n",
    "\n",
    "\n",
    "# First remove puncutation and rows that only have punctuation, e.g. \"&\"\n",
    "social_media_test_2 = social_media_test.copy()\n",
    "social_media_test_2['word'] = social_media_test_2['word'].str.extract('(\\w+)', expand = False)\n",
    "social_media_test_2 = social_media_test_2.dropna().reset_index(drop=True)\n",
    "social_media_test_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Finance SEC-filings\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#finance_train = pd.read_csv('C:/Users/Phil/Sync/entity-recognition-datasets-master/data/SEC-filings/CONLL-format/data/train/FIN5.txt', sep=\" \", header=None)\n",
    "# finance_train = pd.read_csv('~/Sync/entity-recognition-datasets-master/data/SEC-filings/CONLL-format/data/train/FIN5.txt', sep=\" - \", header=None)\n",
    "finance_train = pd.read_csv(\"C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data/SEC-filings/CONLL-format/data/train/FIN5.txt\", sep=\" - \", header=None) \n",
    "\n",
    "# finance_train = pd.read_csv('', sep=\" - \", header=None)\n",
    "\n",
    "finance_train = finance_train.reset_index()\n",
    "finance_train.columns = ['word_tag','label']\n",
    "\n",
    "finance_train.replace(to_replace=[None], value=np.nan, inplace=True)\n",
    "finance_train = finance_train.dropna()\n",
    "finance_train = finance_train.reset_index(drop=True)\n",
    "\n",
    "word_tag_list = pd.DataFrame()\n",
    "for n,word_tag in enumerate(finance_train['word_tag']):\n",
    "    word = word_tag.split(' ')[0]\n",
    "    tag = word_tag.split(' ')[1]\n",
    "    word_tag_list = word_tag_list.append(pd.DataFrame({'word':word,\n",
    "                                                      'tag':tag}, index =[n]))\n",
    "    \n",
    "finance_train['word'] = word_tag_list['word']\n",
    "finance_train['tag'] = word_tag_list['tag']\n",
    "finance_train = finance_train.iloc[:,[2,1]]\n",
    "\n",
    "print(finance_train['label'].unique())\n",
    "\n",
    "\n",
    "# First remove puncutation and rows that only have punctuation, e.g. \"&\"\n",
    "finance_train_2 = finance_train.copy()\n",
    "finance_train_2['word'] = finance_train_2['word'].str.extract('(\\w+)', expand = False)\n",
    "finance_train_2 = finance_train_2.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#finance_train['label'] = np.where(finance_train['label']=='O','O','tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#finance_test = pd.read_csv('C:/Users/Phil/Sync/entity-recognition-datasets-master/data/SEC-filings/CONLL-format/data/test/FIN3.txt', sep=\" \", header=None)\n",
    "# finance_test = pd.read_csv('~/Sync/entity-recognition-datasets-master/data/SEC-filings/CONLL-format/data/test/FIN3.txt', sep=\" - \", header=None)\n",
    "finance_test = pd.read_csv('C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data/SEC-filings/CONLL-format/data/test/FIN3.txt', sep=\" - \", header=None)\n",
    "\n",
    "\n",
    "finance_test = finance_test.reset_index()\n",
    "finance_test.columns = ['word_tag','label']\n",
    "\n",
    "finance_test.replace(to_replace=[None], value=np.nan, inplace=True)\n",
    "finance_test = finance_test.dropna()\n",
    "finance_test = finance_test.reset_index(drop=True)\n",
    "\n",
    "word_tag_list = pd.DataFrame()\n",
    "for n,word_tag in enumerate(finance_test['word_tag']):\n",
    "    word = word_tag.split(' ')[0]\n",
    "    tag = word_tag.split(' ')[1]\n",
    "    word_tag_list = word_tag_list.append(pd.DataFrame({'word':word,\n",
    "                                                      'tag':tag}, index =[n]))\n",
    "    \n",
    "finance_test['word'] = word_tag_list['word']\n",
    "finance_test['tag'] = word_tag_list['tag']\n",
    "finance_test = finance_test.iloc[:,[2,1]]\n",
    "\n",
    "print(finance_test['label'].unique())\n",
    "# First remove puncutation and rows that only have punctuation, e.g. \"&\"\n",
    "finance_test_2 = finance_test.copy()\n",
    "finance_test_2['word'] = finance_test_2['word'].str.extract('(\\w+)', expand = False)\n",
    "finance_test_2 = finance_test_2.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Anatomical AnEM (labels do not map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#anem_train = pd.read_csv('C:/Users/Phil/Sync/entity-recognition-datasets-master/data/AnEM/CONLL-format/data/AnEM.train', sep=\"\\t\", header=None)\n",
    "# anem_train = pd.read_csv('~/Sync/entity-recognition-datasets-master/data/AnEM/CONLL-format/data/AnEM.train', sep=\"\\t\", header=None)\n",
    "anem_train = pd.read_csv('C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data/AnEM/CONLL-format/data/AnEM.train', sep=\"\\t\", header=None)\n",
    "# C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data\n",
    "\n",
    "anem_train = anem_train.iloc[:,[0,3]]\n",
    "anem_train.columns = ['word','label']\n",
    "\n",
    "# First remove puncutation and rows that only have punctuation, e.g. \"&\"\n",
    "anem_train_2 = anem_train.copy()\n",
    "anem_train_2['word'] = anem_train_2['word'].str.extract('(\\w+)', expand = False)\n",
    "anem_train_2 = anem_train_2.dropna().reset_index(drop=True)\n",
    "\n",
    "print(anem_train_2['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#anem_train_2['label'] = np.where(anem_train_2['label']=='O','O','tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#anem_test = pd.read_csv('C:/Users/Phil/Sync/entity-recognition-datasets-master/data/AnEM/CONLL-format/data/AnEM.test', sep=\"\\t\", header=None)\n",
    "# anem_test = pd.read_csv('~/Sync/entity-recognition-datasets-master/data/AnEM/CONLL-format/data/AnEM.test', sep=\"\\t\", header=None)\n",
    "anem_test = pd.read_csv('C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data/AnEM/CONLL-format/data/AnEM.test', sep=\"\\t\", header=None)\n",
    "\n",
    "# C:/data/Projects/Manchester_Uni_of/Trasportability/BERT_tutorial/Data/entity-recognition-datasets-master/data\n",
    "anem_test = anem_test.iloc[:,[0,3]]\n",
    "anem_test.columns = ['word','label']\n",
    "\n",
    "# First remove puncutation and rows that only have punctuation, e.g. \"&\"\n",
    "anem_test_2 = anem_test.copy()\n",
    "anem_test_2['word'] = anem_test_2['word'].str.extract('(\\w+)', expand = False)\n",
    "anem_test_2 = anem_test_2.dropna().reset_index(drop=True)\n",
    "\n",
    "print(anem_test_2['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#anem_test_2['label'] = np.where(anem_test_2['label']=='O','O','tag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Pipeline \n",
    "\n",
    "**Inputs:** \n",
    "\n",
    "1. Source_x,\n",
    "2. Source_y,\n",
    "3. Target_x, \n",
    "4. Target_y\n",
    "\n",
    "**Training:**\n",
    "\n",
    "1. Build a *Dictionary Vectorizer* on Source_x\n",
    "2. Extract labels from Source_y\n",
    "3. Split Source_x & Source_y into an internal train/test split randomly (80/20%)\n",
    "4. Fit a Multinomial NB model (alpha = 0.01) to this internal train set\n",
    "5. Evaluate this on the internal test set for the **Training Results (precision/recall/F1)**\n",
    "\n",
    "**Application to Target data**\n",
    "\n",
    "1. Compute **Feature Gradient Measure** as proportion of unique words from Target_x that appear in training Source_x\n",
    "2. Fit Target_x to Source_x's *Dictionary Vectorizer*\n",
    "3. Apply trained NB model to this\n",
    "4. Evaluate **Applied Results** **(precision/recall/F1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def NB_NER_Pipeline_2(source_x, source_y, source_name, target_x, target_y, target_name, num_samples, model):\n",
    "    import sklearn\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.linear_model import Perceptron\n",
    "\n",
    "    # Establish training classes\n",
    "    X_source = source_x\n",
    "    v_source = DictVectorizer(sparse=False)\n",
    "    X_source = v_source.fit_transform(X_source.to_dict('records'))\n",
    "    y_source = source_y.values\n",
    "    \n",
    "    classes_source = np.unique(y_source)\n",
    "    classes_source = classes_source.tolist()\n",
    "    \n",
    "    if model == 'NB':\n",
    "        # NB classifier trained on source\n",
    "        X_train_source, X_test_source, y_train_source, y_test_source = train_test_split(X_source, y_source, test_size = 0.2, random_state=0)\n",
    "        nb_source = MultinomialNB(alpha=0.01)\n",
    "        nb_source.partial_fit(X_train_source, y_train_source, classes_source)\n",
    "        y_pred_source = nb_source.predict(X_test_source)\n",
    "        output_source = pd.DataFrame({'pred':y_pred_source, 'actual':y_test_source})\n",
    "        precision_training = sklearn.metrics.precision_score(output_source['actual'],output_source['pred'], average='weighted')\n",
    "        recall_training = sklearn.metrics.recall_score(output_source['actual'],output_source['pred'], average='weighted')   \n",
    "        F1_training = 2 * (precision_training * recall_training) / (precision_training + recall_training)\n",
    "    \n",
    "    elif model=='Perceptron':\n",
    "    \n",
    "        # Perceptron classifier trained on source\n",
    "        X_train_source, X_test_source, y_train_source, y_test_source = train_test_split(X_source, y_source, test_size = 0.2, random_state=0)\n",
    "        per_source = Perceptron(verbose=10, n_jobs=-1, max_iter=5)\n",
    "        per_source.partial_fit(X_train_source, y_train_source, classes_source)\n",
    "        y_pred_source = per_source.predict(X_test_source)\n",
    "        output_source = pd.DataFrame({'pred':y_pred_source, 'actual':y_test_source})\n",
    "        precision_training = sklearn.metrics.precision_score(output_source[output_source['actual']!='O']['actual'],\n",
    "                                                             output_source[output_source['actual']!='O']['pred'], average='weighted')\n",
    "        recall_training = sklearn.metrics.recall_score(output_source[output_source['actual']!='O']['actual'],output_source[output_source['actual']!='O']['pred'], average='weighted')   \n",
    "        F1_training = 2 * (precision_training * recall_training) / (precision_training + recall_training)\n",
    "    \n",
    "    \n",
    "    source_x['counter'] = 1\n",
    "    source_x['label'] = source_y\n",
    "    # Sampling target data by num_samples\n",
    "    sampling_output = pd.DataFrame()\n",
    "    for sample in range(0, num_samples):\n",
    "        target_x_sample = target_x.iloc[int((sample/num_samples)*len(target_x))  : int(((sample+1)/num_samples)*len(target_x)),:]\n",
    "        target_y_sample = target_y.iloc[int((sample/num_samples)*len(target_x))  : int(((sample+1)/num_samples)*len(target_x))]\n",
    "    \n",
    "        # Compute measure\n",
    "        target_x_sample_measure = target_x_sample\n",
    "        target_x_sample_measure['label'] = target_y_sample\n",
    "        target_x_sample_measure = target_x_sample_measure[target_x_sample_measure['label']!='O'].merge(source_x, how='left', on=['word','label'])#.drop_duplicates(['word','label']) #SHOULD WE COMPUTER WITHOUT 'O' TAGS???\n",
    "        sample_measure = 1-(target_x_sample_measure['counter'].isna().sum()/len(target_x_sample_measure))\n",
    "    \n",
    "        # Source to Target model\n",
    "        X_source_target = v_source.transform(target_x_sample.to_dict('records'))\n",
    "        y_target_actual = target_y_sample\n",
    "        \n",
    "        if model == 'NB':\n",
    "            y_source_target_pred=nb_source.predict(X_source_target)\n",
    "        elif model=='Perceptron':\n",
    "            y_source_target_pred=per_source.predict(X_source_target)\n",
    "\n",
    "        \n",
    "        output_source_target = pd.DataFrame({'pred':y_source_target_pred, 'actual':y_target_actual})\n",
    "        # Fix applied results to be training results if source is same as target\n",
    "        if source_name == target_name:\n",
    "            precision_applied = precision_training\n",
    "            recall_applied = recall_training\n",
    "        else:\n",
    "            precision_applied = sklearn.metrics.precision_score(output_source_target[output_source_target['actual']!='O']['actual'],\n",
    "                                                                output_source_target[output_source_target['actual']!='O']['pred'], average='weighted')\n",
    "            recall_applied = sklearn.metrics.recall_score(output_source_target[output_source_target['actual']!='O']['actual'],\n",
    "                                                      output_source_target[output_source_target['actual']!='O']['pred'], average='weighted')\n",
    "        F1_applied = 2 * (precision_applied * recall_applied) / (precision_applied + recall_applied)\n",
    "\n",
    "        sampling_output = sampling_output.append(pd.DataFrame({\n",
    "                                                               'Source':source_name,\n",
    "                                                               'Target':target_name,\n",
    "                                                               'sample':(sample+1),\n",
    "                                                               'measure':sample_measure,\n",
    "                                                               'train_prec':precision_training,\n",
    "                                                               'train_rec':recall_training,\n",
    "                                                               'train_F1':F1_training,\n",
    "                                                               'applied_prec':precision_applied,\n",
    "                                                               'applied_rec':recall_applied,\n",
    "                                                               'applied_F1':F1_applied,\n",
    "                                                               'adjusted_F1':(F1_applied)/F1_training\n",
    "                                                               }, index=[sample]))\n",
    "\n",
    "    return(sampling_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "source_x = wiki.drop('label', axis=1)\n",
    "source_y = wiki['label']\n",
    "source_name = 'Wiki'\n",
    "\n",
    "\n",
    "target_x = social_media_test_2.drop('label', axis=1)\n",
    "target_y = social_media_test_2['label']\n",
    "target_name = 'Social Media: test'\n",
    "\n",
    "num_samples = 50\n",
    "\n",
    "model = 'NB'\n",
    "\n",
    "Mdl2 = NB_NER_Pipeline_2(source_x, source_y, source_name, target_x, target_y, target_name, num_samples, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "source_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "Mdl2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(Mdl2['measure'],Mdl2['applied_F1'])\n",
    "plt.title(model+' Transportability Measure against \\n F1 score when Trained on Source applied to Target')\n",
    "plt.xlabel('Transportabiltiy Measure')\n",
    "plt.ylabel('Applied Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(Mdl2['measure'],Mdl2['applied_F1'])\n",
    "plt.title(model+' Transportability Measure against \\n F1 score when Trained on Source applied to Target')\n",
    "plt.xlabel('Transportabiltiy Measure')\n",
    "plt.ylabel('Applied Accuracy')\n",
    "plt.ylim([0,0.6])\n",
    "plt.xlim([0,0.6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "regression = pd.DataFrame()\n",
    "regression['measure'] = Mdl2['measure']\n",
    "regression['applied_F1'] = Mdl2['applied_F1']\n",
    "regression = regression.dropna()\n",
    "from scipy.stats import linregress\n",
    "linregress(regression['measure'],regression['applied_F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "regression = regression.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching Source/Target Systematically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "datasets = [wiki_train, wiki_test, social_media_train_2, social_media_test_2, finance_train_2, finance_test_2, anem_train_2, anem_test_2]\n",
    "dataset_names = ['Wikigold_train', 'Wikigold_test', 'Twitter_train', 'Twitter_test', 'SEC-filings_train', 'SEC-filings_test', 'Anem_train', 'Anem_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "EDA_output = pd.DataFrame()\n",
    "\n",
    "for n1,dataset in enumerate(datasets):\n",
    "    dataset_EDA = pd.DataFrame({'dataset': dataset_names[n1], 'size':len(dataset), 'vocab size':len(dataset['word'].unique()),\n",
    "                               'vocab ratio':len(dataset['word'].unique())/len(dataset),\n",
    "                               'NER_words':len(dataset[dataset['label']!='O']['word']) ,\n",
    "                               'NER_words_unique':len(dataset[dataset['label']!='O']['word'].unique()),\n",
    "                                \n",
    "                               'NER_words_ratio':len(dataset[dataset['label']!='O']['word'].unique())/len(dataset['word'].unique())\n",
    "                               \n",
    "                               }, index = [n1])\n",
    "    EDA_output = EDA_output.append(dataset_EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "EDA_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(EDA_output['dataset'],EDA_output['size'])\n",
    "plt.title(\"Size of Datasets\")\n",
    "plt.ylabel(\"Number of Words\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(EDA_output['dataset'],EDA_output['vocab size'])\n",
    "plt.title(\"Vocab Size of Datasets\")\n",
    "plt.ylabel(\"Number of Unique Words\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(EDA_output['dataset'],EDA_output['NER_words'])\n",
    "plt.title(\"NER Words in Datasets\")\n",
    "plt.ylabel(\"Number of NER Words\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(EDA_output['dataset'],EDA_output['NER_words_unique'])\n",
    "plt.title(\"NER Unique Words of Datasets\")\n",
    "plt.ylabel(\"Number of Unique NER Words\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "num_samples = 1\n",
    "model = 'NB'\n",
    "\n",
    "combined_output = pd.DataFrame()\n",
    "for n1,source in enumerate(datasets):\n",
    "    for n2,target in enumerate(datasets):\n",
    "\n",
    "            source_x = source.drop('label', axis=1)\n",
    "            source_y = source['label']\n",
    "            source_name = dataset_names[n1]\n",
    "            \n",
    "\n",
    "            target_x = target.drop('label', axis=1)\n",
    "            target_y = target['label']\n",
    "            target_name = dataset_names[n2]\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(\"Source Datasets Progress: \", np.round((n1+1)/len(datasets),4)*100, \"%\")\n",
    "            print(\"Source: \", source_name)\n",
    "            print(\"Target Datasets Progress: \", np.round((n2+1)/len(datasets),4)*100, \"%\")\n",
    "            print(\"Target: \", target_name)\n",
    "            \n",
    "\n",
    "            Mdl = NB_NER_Pipeline_2(source_x, source_y, source_name, target_x, target_y, target_name, num_samples, model)\n",
    "            combined_output = combined_output.append(Mdl).reset_index(drop=True)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "EDA_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output['measure'] = combined_output['measure'].round(decimals=2)\n",
    "combined_output['train_prec'] = combined_output['train_prec'].round(decimals=2)\n",
    "combined_output['train_rec'] = combined_output['train_rec'].round(decimals=2)\n",
    "combined_output['train_F1'] = combined_output['train_F1'].round(decimals=2)\n",
    "combined_output['applied_prec'] = combined_output['applied_prec'].round(decimals=2)\n",
    "combined_output['applied_rec'] = combined_output['applied_rec'].round(decimals=2)\n",
    "combined_output['applied_F1'] = combined_output['applied_F1'].round(decimals=2)\n",
    "combined_output['adjusted_F1'] = combined_output['adjusted_F1'].round(decimals=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output_piv = combined_output.replace(0,np.nan).pivot(\"Source\", \"Target\", \"measure\")\n",
    "ax = sns.heatmap(combined_output_piv, annot=True, cbar=False, cmap='Greys')\n",
    "plt.title(\"Feature Gradient\")\n",
    "ax.invert_yaxis()\n",
    "ax.invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output_piv = combined_output.pivot(\"Source\", \"Target\", \"train_F1\")\n",
    "ax = sns.heatmap(combined_output_piv, annot=True, cbar=False)\n",
    "plt.title(\"Training F1 Score\")\n",
    "ax.invert_yaxis()\n",
    "ax.invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output_piv = combined_output.pivot(\"Source\", \"Target\", \"applied_F1\")\n",
    "ax = sns.heatmap(combined_output_piv, annot=True, cbar=False,cmap='Greys')\n",
    "plt.title(\"Applied F1 Score\")\n",
    "ax.invert_yaxis()\n",
    "ax.invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output_piv = combined_output.pivot(\"Source\", \"Target\", \"adjusted_F1\")\n",
    "ax = sns.heatmap(combined_output_piv, annot=True, cbar=False, cmap='Greys')\n",
    "plt.title(r\"$\\tau_p$\" + \" F1 Score \\n (Applied Score given Training Score)\")\n",
    "ax.invert_yaxis()\n",
    "ax.invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(combined_output[combined_output['Source']!=combined_output['Target']]['measure'],combined_output[combined_output['Source']!=combined_output['Target']]['adjusted_F1'])\n",
    "plt.title('Correlation of Feature Gradient Measure against ' + r\"$\\tau_p$\")\n",
    "plt.xlabel('Feature Gradient')\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,1)\n",
    "plt.ylabel(r\"$\\tau_p$\" + ' F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "regression = pd.DataFrame()\n",
    "regression['measure'] = combined_output[combined_output['Source']!=combined_output['Target']]['measure']\n",
    "regression['applied_F1'] = combined_output[combined_output['Source']!=combined_output['Target']]['adjusted_F1']\n",
    "regression = regression.dropna()\n",
    "from scipy.stats import linregress\n",
    "linregress(regression['measure'],regression['applied_F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(combined_output[(combined_output['measure']!=1)&(combined_output['applied_F1']<0.5)]['measure'],\n",
    "            combined_output[(combined_output['measure']!=1)&(combined_output['applied_F1']<0.5)]['adjusted_F1'])\n",
    "plt.title('Correlation of Feature Gradient Measure against ' + r\"$\\tau_p$\")\n",
    "plt.xlabel('Feature Gradient')\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,1)\n",
    "plt.ylabel(r\"$\\tau_p$\" + ' F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "regression = pd.DataFrame()\n",
    "regression['measure'] = combined_output[(combined_output['measure']!=1)&(combined_output['applied_F1']<0.5)]['measure']\n",
    "regression['applied_F1'] = combined_output[(combined_output['measure']!=1)&(combined_output['applied_F1']<0.5)]['adjusted_F1']\n",
    "regression = regression.dropna()\n",
    "from scipy.stats import linregress\n",
    "linregress(regression['measure'],regression['applied_F1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness Pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Inputs:** \n",
    "\n",
    "1. Source_x,\n",
    "2. Source_y,\n",
    "3. Target_x, \n",
    "4. Target_y\n",
    "\n",
    "**Training:**\n",
    "\n",
    "1. Build a *Dictionary Vectorizer* on Source_x\n",
    "2. Extract labels from Source_y\n",
    "3. Split Source_x & Source_y into an internal train/test split randomly (80/20%)\n",
    "4. Fit a Multinomial NB model (alpha = 0.01) to this internal train set\n",
    "5. Evaluate this on the internal test set for the **Training Results (precision/recall/F1)**\n",
    "\n",
    "**Application to Target data**\n",
    "\n",
    "1. Compute **Feature Gradient Measure** as proportion of unique words from Target_x that appear in training Source_x\n",
    "2. Fit Target_x to Source_x's *Dictionary Vectorizer*\n",
    "3. Apply trained NB model to this\n",
    "4. Evaluate **Applied Results** **(precision/recall/F1)**\n",
    "\n",
    "\n",
    "$$Robustness = \\bigg(1+\\frac{1}{4n}\\bigg)\\frac{\\sqrt{\\frac{\\sum_i(\\textrm{perf}(\\Psi,T,\\mathcal{D}'_i)-\\mean{\\textrm{perf}(\\Psi,T,\\mathcal{D})})^2}{n-1}}}{\\mean{\\textrm{perf}(\\Psi,T,\\mathcal{D})}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def robustness(source_x, source_y, source_name, num_samples, model):\n",
    "    import sklearn\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.linear_model import Perceptron\n",
    "\n",
    "    # Establish training classes\n",
    "    X_source = source_x\n",
    "    v_source = DictVectorizer(sparse=False)\n",
    "    X_source = v_source.fit_transform(X_source.to_dict('records'))\n",
    "    y_source = source_y.values\n",
    "    \n",
    "    classes_source = np.unique(y_source)\n",
    "    classes_source = classes_source.tolist()\n",
    "    \n",
    "    sample_output = pd.DataFrame()\n",
    "    for sample in range(0,num_samples):\n",
    "    \n",
    "        if model == 'NB':\n",
    "            # NB classifier trained on source\n",
    "            X_train_source, X_test_source, y_train_source, y_test_source = train_test_split(X_source, y_source, test_size = 0.2)\n",
    "            nb_source = MultinomialNB(alpha=0.01)\n",
    "            nb_source.partial_fit(X_train_source, y_train_source, classes_source)\n",
    "            y_pred_source = nb_source.predict(X_test_source)\n",
    "            output_source = pd.DataFrame({'pred':y_pred_source, 'actual':y_test_source})\n",
    "            precision_training = sklearn.metrics.precision_score(output_source['actual'],output_source['pred'], average='weighted')\n",
    "            recall_training = sklearn.metrics.recall_score(output_source['actual'],output_source['pred'], average='weighted')   \n",
    "            F1_training = 2 * (precision_training * recall_training) / (precision_training + recall_training)\n",
    "\n",
    "        elif model=='Perceptron':\n",
    "\n",
    "            # Perceptron classifier trained on source\n",
    "            X_train_source, X_test_source, y_train_source, y_test_source = train_test_split(X_source, y_source, test_size = 0.2)\n",
    "            per_source = Perceptron(verbose=10, n_jobs=-1, max_iter=5)\n",
    "            per_source.partial_fit(X_train_source, y_train_source, classes_source)\n",
    "            y_pred_source = per_source.predict(X_test_source)\n",
    "            output_source = pd.DataFrame({'pred':y_pred_source, 'actual':y_test_source})\n",
    "            precision_training = sklearn.metrics.precision_score(output_source[output_source['actual']!='O']['actual'],\n",
    "                                                                 output_source[output_source['actual']!='O']['pred'], average='weighted')\n",
    "            recall_training = sklearn.metrics.recall_score(output_source[output_source['actual']!='O']['actual'],output_source[output_source['actual']!='O']['pred'], average='weighted')   \n",
    "            F1_training = 2 * (precision_training * recall_training) / (precision_training + recall_training)\n",
    "\n",
    "        sample_output = sample_output.append(pd.DataFrame({'sample':sample, 'F1':F1_training}, index = [sample]))\n",
    "    mean_F1 = sample_output['F1'].mean()\n",
    "    sample_output['mean_F1'] = mean_F1\n",
    "    sample_output['F1_-_mean_F1_squared'] = np.square(sample_output['F1']-sample_output['mean_F1'])\n",
    "    n = len(sample_output)\n",
    "    robustness = (1 + (1/(4*n))) * ( np.sqrt( (sample_output['F1_-_mean_F1_squared'].sum())/(n-1)  ) /mean_F1 )\n",
    "    return(robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "model = 'NB'\n",
    "\n",
    "robustness_output = pd.DataFrame()\n",
    "for n1,source in enumerate(datasets):\n",
    "    source_x = source.drop('label', axis=1)\n",
    "    source_y = source['label']\n",
    "    source_name = dataset_names[n1]\n",
    "\n",
    "\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Source Datasets Progress: \", np.round((n1+1)/len(datasets),4)*100, \"%\")\n",
    "    print(\"Source: \", source_name)\n",
    "\n",
    "\n",
    "    Mdl = robustness(source_x, source_y, source_name, num_samples, model)\n",
    "    robustness_output = robustness_output.append(pd.DataFrame({'Source': source_name, 'Robustness':Mdl},index = [n1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "robustness_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "robustness_output['Robustness_norm'] = (robustness_output['Robustness'] - min( robustness_output['Robustness']))/(max( robustness_output['Robustness'])-min( robustness_output['Robustness']))\n",
    "robustness_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(robustness_output['Source'], robustness_output['Robustness'])\n",
    "plt.title(\"Robustness of Sources with 10 Samples\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Stability \n",
    "\n",
    "$$\\textrm{Context Stability } \\tau_s = \\bigg(1+\\frac{1}{4n}\\bigg) \\frac{\\sqrt{\\frac{\\sum_{i=1}^n(\\tau_p(\\mathcal{D}_0,\\mathcal{D}_i)-\\mean{\\tau_p})^2}{n-1}}}{\\mean{\\tau_p}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "context_stability_output = pd.DataFrame()\n",
    "for n1,source in enumerate(datasets):\n",
    "    source_name = dataset_names[n1]\n",
    "    source_results = combined_output[combined_output['Source']==source_name].dropna()\n",
    "    source_results = source_results[source_results['Source']!=source_results['Target']]\n",
    "    mean_applied_F1 = source_results['applied_F1'].mean()\n",
    "    source_results['mean_applied_F1'] = mean_applied_F1\n",
    "    source_results['applied_F1_-_mean_applied_F1_squared'] = np.square(source_results['applied_F1']-source_results['mean_applied_F1'])\n",
    "    \n",
    "    n = len(source_results)\n",
    "    context_stability = (1 + (1/(4*n))) * ( np.sqrt( (source_results['applied_F1_-_mean_applied_F1_squared'].sum())/(n-1)  ) /mean_applied_F1 )\n",
    "    context_stability_output = context_stability_output.append(pd.DataFrame({'Source': source_name, 'Context stability':context_stability},index = [n1]))\n",
    "context_stability_output = context_stability_output.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "context_stability_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "context_stability_output['Context stability_norm'] = (context_stability_output['Context stability'] - min( context_stability_output['Context stability']))/(max( context_stability_output['Context stability'])-min( context_stability_output['Context stability']))\n",
    "context_stability_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(context_stability_output['Source'], context_stability_output['Context stability'])\n",
    "plt.title(\"Stability of Sources\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output_2 = combined_output.merge(context_stability_output, how='left', on = 'Source')\n",
    "combined_output_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output_3 = combined_output_2.merge(robustness_output, how='left', on = 'Source')\n",
    "combined_output_3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output_3[['Source', 'Target','measure','applied_F1','adjusted_F1','Context stability', 'Robustness']][0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Vec Mapping Analysis\n",
    "\n",
    "following guide found here: \n",
    "\n",
    "http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "\"Using Googleâ€™s pre-trained model. Itâ€™s 1.5GB! It includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. The vector length is 300 features.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('~/Sync/entity-recognition-datasets-master/GoogleW2V/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "w2v_model['queen'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "len(w2v_model['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "w2v_model['is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "wiki_word_list = pd.DataFrame()\n",
    "for n,word in enumerate(wiki['word']):\n",
    "    clear_output(wait=True)\n",
    "    print(\"Words Completed: \", np.round(n/len(wiki),4)*100,\"%\")\n",
    "    try:\n",
    "        word_vector = w2v_model[str(word)]\n",
    "    except:\n",
    "        word_vector = \"missing\" #np.zeros(300, dtype=np.float32)\n",
    "    wiki_word_list = wiki_word_list.append(pd.DataFrame({'vector':[word_vector]}, index = [n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "wiki['vector'] = wiki_word_list['vector']\n",
    "wiki.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "wiki[wiki['label']!='O'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Percentage of words missing in vector lookup = \",(len(wiki[wiki['vector']==\"missing\"])/len(wiki))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Percentage of NER words missing in vector lookup = \",(len(wiki[(wiki['vector']==\"missing\")&(wiki['label']!='O')])/len(wiki[wiki['label']!='O']))*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for these missing lookup words, I will simply set the vector to a list of 0s if missing for now. \n",
    "\n",
    "First, I repeat this lookup process for each dataset.\n",
    "\n",
    "Also need to normalise the values to be positive as NB won't accept negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for d,dataset in enumerate(datasets):\n",
    "    vector_word_list = pd.DataFrame()\n",
    "    for n,word in enumerate(datasets[d]['word']):\n",
    "        clear_output(wait=True)\n",
    "        print(\"DATASET: \", dataset_names[d])\n",
    "        print(\"Dataset: \", np.round(d/len(dataset_names),4)*100, \"%\")\n",
    "        print(\"Words Completed: \", np.round(n/len(datasets[d]),4)*100,\"%\")\n",
    "        try:\n",
    "            word_vector = w2v_model[str(word)]\n",
    "            #Normalise [-1:1] to [0:1], replace outliers that are less than -1 originally\n",
    "            #word_vector = (word_vector - (-1))/(1 - (-1))\n",
    "            #word_vector = [0 if i <0 else i for i in word_vector]\n",
    "        except:\n",
    "            word_vector = np.zeros(300, dtype=np.float32)\n",
    "        vector_word_list = vector_word_list.append(pd.DataFrame({'vector':[word_vector]}, index = [n]))\n",
    "    \n",
    "    datasets[d]['vector'] = vector_word_list['vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "datasets[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def NB_NER_Pipeline_3(source_x, source_y, source_name, target_x, target_y, target_name, num_samples, model):\n",
    "    import sklearn\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.linear_model import Perceptron\n",
    "\n",
    "    # Establish training classes\n",
    "    X_source = np.array(source_x['vector'].tolist())\n",
    "    # ALREADY TRANSFORMED BY LOOKUP\n",
    "    #v_source = DictVectorizer(sparse=False)\n",
    "    #X_source = v_source.fit_transform(X_source.to_dict('records'))\n",
    "    y_source = source_y.values\n",
    "    \n",
    "    classes_source = np.unique(y_source)\n",
    "    classes_source = classes_source.tolist()\n",
    "    \n",
    "    if model == 'NB':\n",
    "        # NB classifier trained on source\n",
    "        X_train_source, X_test_source, y_train_source, y_test_source = train_test_split(X_source, y_source, test_size = 0.2, random_state=0)\n",
    "        nb_source = MultinomialNB(alpha=0.01)\n",
    "        nb_source.partial_fit(X_train_source, y_train_source, classes_source)\n",
    "        y_pred_source = nb_source.predict(X_test_source)\n",
    "        output_source = pd.DataFrame({'pred':y_pred_source, 'actual':y_test_source})\n",
    "        precision_training = sklearn.metrics.precision_score(output_source['actual'],output_source['pred'], average='weighted')\n",
    "        recall_training = sklearn.metrics.recall_score(output_source['actual'],output_source['pred'], average='weighted')   \n",
    "        F1_training = 2 * (precision_training * recall_training) / (precision_training + recall_training)\n",
    "    \n",
    "    elif model=='Perceptron':\n",
    "    \n",
    "        # Perceptron classifier trained on source\n",
    "        X_train_source, X_test_source, y_train_source, y_test_source = train_test_split(X_source, y_source, test_size = 0.2, random_state=0)\n",
    "        per_source = Perceptron(verbose=0, n_jobs=-1, max_iter=1000, alpha=0.0009)\n",
    "        per_source.partial_fit(X_train_source, y_train_source, classes_source)\n",
    "        y_pred_source = per_source.predict(X_test_source)\n",
    "        output_source = pd.DataFrame({'pred':y_pred_source, 'actual':y_test_source})\n",
    "        precision_training = sklearn.metrics.precision_score(output_source[output_source['actual']!='O']['actual'],\n",
    "                                                             output_source[output_source['actual']!='O']['pred'], average='weighted')\n",
    "        recall_training = sklearn.metrics.recall_score(output_source[output_source['actual']!='O']['actual'],output_source[output_source['actual']!='O']['pred'], average='weighted')   \n",
    "        F1_training = 2 * (precision_training * recall_training) / (precision_training + recall_training)\n",
    "    \n",
    "    \n",
    "    source_x['counter'] = 1\n",
    "    \n",
    "    # Sampling target data by num_samples\n",
    "    sampling_output = pd.DataFrame()\n",
    "    for sample in range(0, num_samples):\n",
    "        target_x_sample = target_x.iloc[int((sample/num_samples)*len(target_x))  : int(((sample+1)/num_samples)*len(target_x)),:]\n",
    "        target_y_sample = target_y.iloc[int((sample/num_samples)*len(target_y))  : int(((sample+1)/num_samples)*len(target_y))]\n",
    "     \n",
    "        # Compute measure\n",
    "        target_x_sample_measure = target_x_sample[target_y_sample!='O'][['word']].merge(source_x[['word','counter']], how='left', on='word').drop_duplicates() #SHOULD WE COMPUTER WITHOUT 'O' TAGS???\n",
    "        sample_measure = 1-(target_x_sample_measure['counter'].isna().sum()/len(target_x_sample_measure))\n",
    "    \n",
    "        # Source to Target model - NOT NEEDED IN AS THIS IS ALREADY PRE-FITTED\n",
    "        #X_source_target = v_source.transform(target_x_sample.to_dict('records'))\n",
    "        X_target = np.array(target_x_sample['vector'].tolist())\n",
    "        y_target_actual = target_y_sample\n",
    "        \n",
    "        if model == 'NB':\n",
    "            y_source_target_pred=nb_source.predict(X_target)\n",
    "        elif model=='Perceptron':\n",
    "            y_source_target_pred=per_source.predict(X_target)\n",
    "\n",
    "        \n",
    "        output_source_target = pd.DataFrame({'pred':y_source_target_pred, 'actual':y_target_actual})\n",
    "        # Fix applied results to be training results if source is same as target\n",
    "        if source_name == target_name:\n",
    "            precision_applied = precision_training\n",
    "            recall_applied = recall_training\n",
    "        else:\n",
    "            precision_applied = sklearn.metrics.precision_score(output_source_target[output_source_target['actual']!='O']['actual'],\n",
    "                                                                output_source_target[output_source_target['actual']!='O']['pred'], average='weighted')\n",
    "            recall_applied = sklearn.metrics.recall_score(output_source_target[output_source_target['actual']!='O']['actual'],\n",
    "                                                      output_source_target[output_source_target['actual']!='O']['pred'], average='weighted')\n",
    "       \n",
    "        F1_applied = 2 * (precision_applied * recall_applied) / (precision_applied + recall_applied)\n",
    "\n",
    "        sampling_output = sampling_output.append(pd.DataFrame({\n",
    "                                                               'Source':source_name,\n",
    "                                                               'Target':target_name,\n",
    "                                                               'sample':(sample+1),\n",
    "                                                               'measure':sample_measure,\n",
    "                                                               'train_prec':precision_training,\n",
    "                                                               'train_rec':recall_training,\n",
    "                                                               'train_F1':F1_training,\n",
    "                                                               'applied_prec':precision_applied,\n",
    "                                                               'applied_rec':recall_applied,\n",
    "                                                               'applied_F1':F1_applied,\n",
    "                                                               'adjusted_F1':(F1_applied)/F1_training\n",
    "                                                               }, index=[sample]))\n",
    "\n",
    "    return(sampling_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "num_samples = 1\n",
    "model = 'Perceptron'\n",
    "\n",
    "combined_output_Vec = pd.DataFrame()\n",
    "for n1,source in enumerate(datasets):\n",
    "    for n2,target in enumerate(datasets):\n",
    "\n",
    "            source_x = source.drop('label', axis=1)\n",
    "            source_y = source['label']\n",
    "            source_name = dataset_names[n1]\n",
    "            \n",
    "\n",
    "            target_x = target.drop('label', axis=1)\n",
    "            target_y = target['label']\n",
    "            target_name = dataset_names[n2]\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(\"Source Datasets Progress: \", np.round((n1+1)/len(datasets),4)*100, \"%\")\n",
    "            print(\"Source: \", source_name)\n",
    "            print(\"Target Datasets Progress: \", np.round((n2+1)/len(datasets),4)*100, \"%\")\n",
    "            print(\"Target: \", target_name)\n",
    "            \n",
    "\n",
    "            Mdl = NB_NER_Pipeline_3(source_x, source_y, source_name, target_x, target_y, target_name, num_samples, model)\n",
    "            combined_output_Vec = combined_output_Vec.append(Mdl).reset_index(drop=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output_piv_Vec = combined_output_Vec.pivot(\"Source\", \"Target\", \"applied_F1\")\n",
    "ax = sns.heatmap(combined_output_piv_Vec, annot=True, cbar=False, cmap='BuPu')\n",
    "plt.title(\"Applied F1 Score (W2V)\")\n",
    "ax.invert_yaxis()\n",
    "ax.invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined_output_Vec[combined_output_Vec['Source']=='SEC-filings_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-c3d61fd3",
   "language": "python",
   "display_name": "PyCharm (Transportation)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}